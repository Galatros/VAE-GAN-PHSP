{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_helper import get_dataloaders_and_standarscaler_photons, get_dataloaders_and_standarscaler_photons_from_numpy\n",
    "from train_helper import train_vae, train_vae_mmd\n",
    "from plot_helper import plot_training_loss\n",
    "from models_architecture_helper import VAE_Linear_0203\n",
    "\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO czy zapisywać Logging interval ? \n",
    "#TODO i czy zapisywać całkowity number of epochs gdyby we wczytanym modelu robić kolejnego checkpointa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "RANDOM_SEED = 123\n",
    "LEARNING_RATE = 0.0005\n",
    "BATCH_SIZE = 10000\n",
    "NUM_EPOCHS = 2\n",
    "LOGGING_INTERVAL=300\n",
    "RECONSTRUCTION_TERM_WEIGHT=1\n",
    "\n",
    "PLOT_FRACTION=0.0125\n",
    "TEST_FRACTION=0.4\n",
    "VALIDATION_FRACTION=0.0\n",
    "SAVE_MODEL_FILE='checkpoint.pth'\n",
    "NUM_WORKERS=0\n",
    "path='/data1/dose-3d-generative/data/training-data/DISP_0.5_ANGLE_0/NUMPY/a1_10_7.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_DEVICE_NUM=0\n",
    "DEVICE = torch.device(f'cuda:{CUDA_DEVICE_NUM}' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.memory_allocated(device=DEVICE))\n",
    "print(torch.cuda.memory_reserved(device=DEVICE))\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ODCZYTANIE DANYCH Z PLIKU 'photons.npy'\n",
    "photons = np.load(path)\n",
    "X = np.zeros((10000001, 6),dtype=np.float32)\n",
    "np.copyto(X,photons[:,:-1])\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reflection=copy.deepcopy(X)\n",
    "X_reflection[:,2]=-X_reflection[:,2]\n",
    "X_reflection[:,4]=-X_reflection[:,4]\n",
    "\n",
    "X_sum=np.concatenate((X,X_reflection),axis=0)\n",
    "print(len(X_sum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.DataFrame(X_sum, columns = ['X', 'Y', 'dX', 'dY', 'dZ', 'E'])\n",
    "df_data.head()#zawsze warto rzucić okiem na dane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_check=df_data.to_numpy(dtype=np.float32)\n",
    "orginal_check=copy.deepcopy(tmp_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 3)\n",
    "fig.set_size_inches(16,8)\n",
    "bins=100\n",
    "axs[0, 0].hist(orginal_check[:,0],bins=bins, label ='orginal',alpha=0.5, density=True)\n",
    "axs[0, 0].set_title('X')\n",
    "axs[0, 1].hist(orginal_check[:,1],bins=bins, label ='orginal',alpha=0.5, density=True)\n",
    "axs[0, 1].set_title('Y')\n",
    "axs[0, 2].hist(orginal_check[:,2],bins=bins, label ='orginal',alpha=0.5, density=True)\n",
    "axs[0, 2].set_title('dX')\n",
    "axs[1, 0].hist(orginal_check[:,3],bins=bins, label ='orginal',alpha=0.5, density=True)\n",
    "axs[1, 0].set_title('dY')\n",
    "axs[1, 1].hist(orginal_check[:,4],bins=bins, label ='orginal',alpha=0.5, density=True)\n",
    "axs[1, 1].set_title('dZ')\n",
    "axs[1, 2].hist(orginal_check[:,5],bins=bins, label ='orginal',alpha=0.5, density=True)\n",
    "axs[1, 2].set_title('E')\n",
    "fig.legend()\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, valid_loader, test_loader, stdcs = get_dataloaders_and_standarscaler_photons_from_numpy(tmp_X=X_sum,\n",
    "    batch_size=BATCH_SIZE, \n",
    "    num_workers=NUM_WORKERS,\n",
    "    test_fraction=TEST_FRACTION, \n",
    "    validation_fraction=VALIDATION_FRACTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAE_Linear_0203()\n",
    "model.to(DEVICE)\n",
    "\n",
    "#criterion = nn.MSELoss()#FUNKCJA STRATY\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                             lr=LEARNING_RATE, \n",
    "                             weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dict=train_vae_mmd(num_epochs=NUM_EPOCHS, device=DEVICE, model=model,optimizer=optimizer,train_loader=train_loader,loss_fn=None, test_loader=test_loader, logging_interval=LOGGING_INTERVAL, reconstruction_term_weight=RECONSTRUCTION_TERM_WEIGHT, save_model_file=SAVE_MODEL_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_loss(log_dict['train_reconstruction_loss_per_batch'], NUM_EPOCHS, custom_label=\" (reconstruction)\")\n",
    "plot_training_loss(log_dict['train_kl_loss_per_batch'], NUM_EPOCHS, custom_label=\" (KL)\")\n",
    "plot_training_loss(log_dict['train_combined_loss_per_batch'], NUM_EPOCHS, custom_label=\" (combined)\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(log_dict['train_combined_loss_per_epoch'])), (log_dict['train_combined_loss_per_epoch']), label='Train Epoch Loss')\n",
    "plt.plot(range(len(log_dict['test_combined_loss_per_epoch'])), (log_dict['test_combined_loss_per_epoch']), label='Test Epoch Loss')\n",
    "plt.legend()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "#plt.ylim(0.15,0.3)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp=df_data.to_numpy(dtype=np.float32)\n",
    "orginal=copy.deepcopy(tmp)\n",
    "tmp=stdcs.transform(tmp)\n",
    "tmp=torch.from_numpy(tmp)\n",
    "#print(tmp)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    result_encoded_features, z_mean, z_log_var, result_decoded_features =model(tmp.to(device=DEVICE))\n",
    "result=result_decoded_features.cpu().detach().numpy()\n",
    "result=stdcs.inverse_transform(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 3)\n",
    "fig.set_size_inches(16,8)\n",
    "bins=100\n",
    "axs[0, 0].hist(orginal[:,0],bins=bins, label ='orginal',alpha=0.5, density=True)\n",
    "axs[0, 0].hist(result[:,0],bins=bins, label ='decoded', alpha=0.5, density=True)\n",
    "axs[0, 0].set_title('X')\n",
    "axs[0, 1].hist(orginal[:,1],bins=bins, label ='orginal',alpha=0.5, density=True)\n",
    "axs[0, 1].hist(result[:,1],bins=bins, label ='decoded', alpha=0.5, density=True)\n",
    "axs[0, 1].set_title('Y')\n",
    "axs[0, 2].hist(orginal[:,2],bins=bins, label ='orginal',alpha=0.5, density=True)\n",
    "axs[0, 2].hist(result[:,2],bins=bins, label ='decoded', alpha=0.5, density=True)\n",
    "axs[0, 2].set_title('dX')\n",
    "axs[1, 0].hist(orginal[:,3],bins=bins, label ='orginal',alpha=0.5, density=True)\n",
    "axs[1, 0].hist(result[:,3],bins=bins, label ='decoded', alpha=0.5, density=True)\n",
    "axs[1, 0].set_title('dY')\n",
    "axs[1, 1].hist(orginal[:,4],bins=bins, label ='orginal',alpha=0.5, density=True)\n",
    "axs[1, 1].hist(result[:,4],bins=bins, label ='decoded', alpha=0.5, density=True)\n",
    "axs[1, 1].set_title('dZ')\n",
    "axs[1, 2].hist(orginal[:,5],bins=bins, label ='orginal',alpha=0.5, density=True)\n",
    "axs[1, 2].hist(result[:,5],bins=bins, label ='decoded', alpha=0.5, density=True)\n",
    "axs[1, 2].set_title('E')\n",
    "fig.legend()\n",
    "\n",
    "# for ax in axs.flat:\n",
    "#     ax.set(xlabel='x-label', ylabel='y-label')\n",
    "\n",
    "# # Hide x labels and tick labels for top plots and y ticks for right plots.\n",
    "# for ax in axs.flat:\n",
    "#     ax.label_outer()\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = df_data.columns\n",
    "fig, axs = plt.subplots(2, 3)\n",
    "fig.set_size_inches(20, 20)\n",
    "for i, j in enumerate(keys):\n",
    "    mi = np.minimum(orginal[:, i].min(), result[:, i].min())\n",
    "    ma = np.maximum(orginal[:, i].max(), result[:, i].max())\n",
    "    bins = np.linspace(mi, ma, 300)\n",
    "    axs.flatten()[i].hist(orginal[:, i], bins, alpha=.5)\n",
    "    axs.flatten()[i].hist(result[:, i], bins, alpha=.5)\n",
    "    axs.flatten()[i].set_title(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_df=pd.DataFrame(result,columns=['X', 'Y', 'dX', 'dY', 'dZ', 'E'])\n",
    "orginal_df=df_data.iloc[:,:]\n",
    "\n",
    "\n",
    "concatenated_datasets=pd.concat([orginal_df.assign(dataset_name='orginal'), decoded_df.assign(dataset_name='decoded')],ignore_index=True)\n",
    "concatenated_datasets.shape\n",
    "print(concatenated_datasets.index.is_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO trzeba by sprawdzić czy to losowanie działa poprawnie i zwraca próbkę reprezentatywną\n",
    "sample_concatenated=concatenated_datasets.groupby('dataset_name', group_keys=False).apply(lambda x: x.sample(frac=PLOT_FRACTION, random_state=2)) \n",
    "print(sample_concatenated.shape)\n",
    "print(sample_concatenated.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_orginal=orginal_df.apply(lambda x: x.sample(frac=PLOT_FRACTION, random_state=2)) \n",
    "print(sample_orginal.shape)\n",
    "print(sample_orginal.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.pairplot(sample_orginal, kind=\"scatter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_decoded=decoded_df.apply(lambda x: x.sample(frac=PLOT_FRACTION, random_state=2)) \n",
    "print(sample_decoded.shape)\n",
    "print(sample_decoded.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.pairplot(sample_decoded, kind=\"scatter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.pairplot(sample_concatenated, hue=\"dataset_name\", kind=\"scatter\", plot_kws=dict(alpha=0.5))\n",
    "\n",
    "# g = sns.PairGrid(concatenated_datasets, hue='dataset_name')\n",
    "# g.map_upper(sns.scatterplot)\n",
    "# #g.map_lower(sns.kdeplot, fill=True)\n",
    "# g.map_diag(sns.histplot, kde=True)\n",
    "\n",
    "# sns.pairplot(concatenated_datasets, hue=\"dataset_name\", kind=\"scatter\", plot_kws=dict(alpha=0.5),corner=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_2=df_data.iloc[37:38,:]\n",
    "tmp_2=tmp_2.to_numpy(dtype=np.float32)\n",
    "orginal_2=tmp_2\n",
    "tmp_2=stdcs.transform(tmp_2)\n",
    "tmp_2=torch.from_numpy(tmp_2)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    result_encoded_features_2, z_mean_2, z_log_var_2, result_decoded_features_2=model(tmp_2.to(device=DEVICE))\n",
    "result_2=result_decoded_features_2.cpu().detach().numpy()\n",
    "result_2=stdcs.inverse_transform(result_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orginal_2=orginal_2.flatten()\n",
    "result_2=result_2.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(np.stack((orginal_2,result_2)), columns=['X', 'Y', 'dX', 'dY', 'dZ', 'E'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded_features=[]\n",
    "# model.eval()\n",
    "# for index, feature in enumerate(train_loader):\n",
    "#     with torch.no_grad():\n",
    "#         tmp_encoded_features, z_mean, z_log_var, decoded =model(feature.to(device=DEVICE))\n",
    "#         encoded_features.extend(tmp_encoded_features.cpu().detach().numpy())\n",
    "# encoded_features=np.asarray(encoded_features)\n",
    "\n",
    "# print(encoded_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #PRZESTRZEŃ UKRYTA VAE\n",
    "# colors = ['r']\n",
    "# markers = ['s']\n",
    "\n",
    "# fig=plt.figure()\n",
    "# ax=fig.add_subplot(projection='3d')\n",
    "# for c, m in zip(colors, markers):\n",
    "#     ax.scatter(encoded_features[:,0],encoded_features[:,2],encoded_features[:,1],marker=m,c=c)\n",
    "\n",
    "# ax.set_xlabel('VAE 1')\n",
    "# ax.set_ylabel('VAE 2')\n",
    "# ax.set_zlabel('VAE 3')\n",
    "\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "94e50cba98016e2c68fceb7b9fa13391593fa96fd78bad5a436d2ca9f0548949"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('pytorch_wine_env': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
