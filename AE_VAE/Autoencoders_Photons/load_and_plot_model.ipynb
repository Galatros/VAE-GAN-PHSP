{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_helper import get_dataloaders_and_standarscaler_photons_from_numpy\n",
    "from plot_helper import plot_training_loss\n",
    "\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "FRACTION_PLOT=0.0125\n",
    "path='/data1/dose-3d-generative/data/training-data/DISP_0.5_ANGLE_0/NUMPY/a1_10_7.npy'\n",
    "CHECKPOINT_PATH=\"checkpoint_mmd.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_checkpoint = torch.load(CHECKPOINT_PATH)\n",
    "NUM_EPOCHS = loaded_checkpoint[\"epoch\"]\n",
    "print(NUM_EPOCHS)\n",
    "model_name=loaded_checkpoint[\"model_name\"]\n",
    "print(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = getattr(__import__('models_architecture_helper', fromlist=[model_name]), model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=model()\n",
    "# model.to(DEVICE)\n",
    "model.load_state_dict(loaded_checkpoint[\"model_state\"])\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                             lr=0, \n",
    "                             weight_decay=1e-5)\n",
    "\n",
    "optimizer.load_state_dict(loaded_checkpoint[\"optim_state\"])\n",
    "BATCH_SIZE = loaded_checkpoint[\"batch_size\"]\n",
    "RECONSTRUCTION_TERM_WEIGHT = loaded_checkpoint[\"reconstruction_term_weight\"]\n",
    "log_dict=loaded_checkpoint[\"log_dict\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ODCZYTANIE DANYCH Z PLIKU 'photons.npy'\n",
    "photons = np.load(path)\n",
    "X = np.zeros((10000001, 6),dtype=np.float32)\n",
    "np.copyto(X,photons[:,:-1])\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reflection=copy.deepcopy(X)\n",
    "X_reflection[:,2]=-X_reflection[:,2]\n",
    "X_reflection[:,4]=-X_reflection[:,4]\n",
    "\n",
    "X_sum=np.concatenate((X,X_reflection),axis=0)\n",
    "print(len(X_sum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.DataFrame(X_sum, columns = ['X', 'Y', 'dX', 'dY', 'dZ', 'E'])\n",
    "df_data.head()#zawsze warto rzucić okiem na dane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, valid_loader, test_loader, stdcs = get_dataloaders_and_standarscaler_photons_from_numpy(tmp_X=X_sum,\n",
    "    batch_size=BATCH_SIZE, \n",
    "    num_workers=0,\n",
    "    test_fraction=0.4, \n",
    "    validation_fraction=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_loss(log_dict['train_reconstruction_loss_per_batch'], NUM_EPOCHS, custom_label=\" (reconstruction)\")\n",
    "plot_training_loss(log_dict['train_kl_loss_per_batch'], NUM_EPOCHS, custom_label=\" (KL)\")\n",
    "plot_training_loss(log_dict['train_combined_loss_per_batch'], NUM_EPOCHS, custom_label=\" (combined)\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(log_dict['train_combined_loss_per_epoch'])), (log_dict['train_combined_loss_per_epoch']), label='Train Epoch Loss')\n",
    "plt.plot(range(len(log_dict['test_combined_loss_per_epoch'])), (log_dict['test_combined_loss_per_epoch']), label='Test Epoch Loss')\n",
    "plt.legend()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "#plt.ylim(0.15,0.3)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp=df_data.to_numpy(dtype=np.float32)\n",
    "orginal=copy.deepcopy(tmp)\n",
    "tmp=stdcs.transform(tmp)\n",
    "tmp=torch.from_numpy(tmp)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    result_encoded_features, z_mean, z_log_var, result_decoded_features =model(tmp)#(tmp.to(device=DEVICE))\n",
    "result=result_decoded_features.cpu().detach().numpy()\n",
    "result=stdcs.inverse_transform(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 3)\n",
    "fig.set_size_inches(16,8)\n",
    "bins=100\n",
    "axs[0, 0].hist(orginal[:,0],bins=bins, label ='orginal',alpha=0.5, density=True)\n",
    "axs[0, 0].hist(result[:,0],bins=bins, label ='decoded', alpha=0.5, density=True)\n",
    "axs[0, 0].set_title('X')\n",
    "axs[0, 1].hist(orginal[:,1],bins=bins, label ='orginal',alpha=0.5, density=True)\n",
    "axs[0, 1].hist(result[:,1],bins=bins, label ='decoded', alpha=0.5, density=True)\n",
    "axs[0, 1].set_title('Y')\n",
    "axs[0, 2].hist(orginal[:,2],bins=bins, label ='orginal',alpha=0.5, density=True)\n",
    "axs[0, 2].hist(result[:,2],bins=bins, label ='decoded', alpha=0.5, density=True)\n",
    "axs[0, 2].set_title('dX')\n",
    "axs[1, 0].hist(orginal[:,3],bins=bins, label ='orginal',alpha=0.5, density=True)\n",
    "axs[1, 0].hist(result[:,3],bins=bins, label ='decoded', alpha=0.5, density=True)\n",
    "axs[1, 0].set_title('dY')\n",
    "axs[1, 1].hist(orginal[:,4],bins=bins, label ='orginal',alpha=0.5, density=True)\n",
    "axs[1, 1].hist(result[:,4],bins=bins, label ='decoded', alpha=0.5, density=True)\n",
    "axs[1, 1].set_title('dZ')\n",
    "axs[1, 2].hist(orginal[:,5],bins=bins, label ='orginal',alpha=0.5, density=True)\n",
    "axs[1, 2].hist(result[:,5],bins=bins, label ='decoded', alpha=0.5, density=True)\n",
    "axs[1, 2].set_title('E')\n",
    "fig.legend()\n",
    "\n",
    "# for ax in axs.flat:\n",
    "#     ax.set(xlabel='x-label', ylabel='y-label')\n",
    "\n",
    "# # Hide x labels and tick labels for top plots and y ticks for right plots.\n",
    "# for ax in axs.flat:\n",
    "#     ax.label_outer()\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = df_data.columns\n",
    "fig, axs = plt.subplots(2, 3)\n",
    "fig.set_size_inches(20, 20)\n",
    "for i, j in enumerate(keys):\n",
    "    mi = np.minimum(orginal[:, i].min(), result[:, i].min())\n",
    "    ma = np.maximum(orginal[:, i].max(), result[:, i].max())\n",
    "    bins = np.linspace(mi, ma, 300)\n",
    "    axs.flatten()[i].hist(orginal[:, i], bins, alpha=.5)\n",
    "    axs.flatten()[i].hist(result[:, i], bins, alpha=.5)\n",
    "    axs.flatten()[i].set_title(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_df=pd.DataFrame(result,columns=['X', 'Y', 'dX', 'dY', 'dZ', 'E'])\n",
    "orginal_df=df_data.iloc[:,:]\n",
    "\n",
    "\n",
    "concatenated_datasets=pd.concat([orginal_df.assign(dataset_name='orginal'), decoded_df.assign(dataset_name='decoded')],ignore_index=True)\n",
    "concatenated_datasets.shape\n",
    "print(concatenated_datasets.index.is_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO trzeba by sprawdzić czy to losowanie działa poprawnie i zwraca próbkę reprezentatywną\n",
    "sample_concatenated=concatenated_datasets.groupby('dataset_name', group_keys=False).apply(lambda x: x.sample(frac=FRACTION_PLOT, random_state=2)) \n",
    "print(sample_concatenated.shape)\n",
    "print(sample_concatenated.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(sample_concatenated, hue=\"dataset_name\", kind=\"scatter\", plot_kws=dict(alpha=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_orginal=orginal_df.apply(lambda x: x.sample(frac=FRACTION_PLOT, random_state=2)) \n",
    "print(sample_orginal.shape)\n",
    "print(sample_orginal.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g=sns.pairplot(sample_orginal, kind=\"scatter\")\n",
    "g.fig.suptitle(\"Orginal\",y=1.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_decoded=decoded_df.apply(lambda x: x.sample(frac=FRACTION_PLOT, random_state=2)) \n",
    "print(sample_decoded.shape)\n",
    "print(sample_decoded.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g2=sns.pairplot(sample_decoded, kind=\"scatter\")\n",
    "g2.fig.suptitle(\"Decoded\",y=1.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_2=df_data.iloc[37:38,:]\n",
    "tmp_2=tmp_2.to_numpy(dtype=np.float32)\n",
    "orginal_2=tmp_2\n",
    "tmp_2=stdcs.transform(tmp_2)\n",
    "tmp_2=torch.from_numpy(tmp_2)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    result_encoded_features_2, z_mean_2, z_log_var_2, result_decoded_features_2=model(tmp_2)#(tmp_2.to(device=DEVICE))\n",
    "result_2=result_decoded_features_2.cpu().detach().numpy()\n",
    "result_2=stdcs.inverse_transform(result_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orginal_2=orginal_2.flatten()\n",
    "result_2=result_2.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(np.stack((orginal_2,result_2)), columns=['X', 'Y', 'dX', 'dY', 'dZ', 'E'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_features=[]\n",
    "model.eval()\n",
    "for index, feature in enumerate(train_loader):\n",
    "    with torch.no_grad():\n",
    "        tmp_encoded_features, z_mean, z_log_var, decoded =model(feature)#(feature.to(device=DEVICE))\n",
    "        encoded_features.extend(tmp_encoded_features.cpu().detach().numpy())\n",
    "encoded_features=np.asarray(encoded_features)\n",
    "\n",
    "print(encoded_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PRZESTRZEŃ UKRYTA VAE\n",
    "colors = ['r']\n",
    "markers = ['s']\n",
    "\n",
    "fig=plt.figure()\n",
    "ax=fig.add_subplot(projection='3d')\n",
    "for c, m in zip(colors, markers):\n",
    "    ax.scatter(encoded_features[:,0],encoded_features[:,1],encoded_features[:,2],marker=m,c=c)\n",
    "\n",
    "ax.set_xlabel('VAE 1')\n",
    "ax.set_ylabel('VAE 2')\n",
    "ax.set_zlabel('VAE 3')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "94e50cba98016e2c68fceb7b9fa13391593fa96fd78bad5a436d2ca9f0548949"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('pytorch_wine_env': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
